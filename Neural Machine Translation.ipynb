{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc22aa0e",
   "metadata": {},
   "source": [
    "Your Task is to:\n",
    "1. Build a Neural Machine Translation model.\n",
    "2. Evaluate your model using BLEU score.\n",
    "\n",
    "Dataset: http://www.manythings.org/anki/fra-eng.zip\n",
    "\n",
    "Notes\n",
    "1. Refer notes for hint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b8fa0",
   "metadata": {},
   "source": [
    "## Downloading the ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670b4821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-03-19 22:17:09--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 2606:4700:8d7e:7d56:abdd:10:b19e:a55c, 172.67.186.54, 104.21.92.44\n",
      "Connecting to www.manythings.org (www.manythings.org)|2606:4700:8d7e:7d56:abdd:10:b19e:a55c|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6532197 (6.2M) [application/zip]\n",
      "Saving to: 'fra-eng.zip'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 3.93M 2s\n",
      "    50K .......... .......... .......... .......... ..........  1%  709K 5s\n",
      "   100K .......... .......... .......... .......... ..........  2%  762K 6s\n",
      "   150K .......... .......... .......... .......... ..........  3% 1.32M 6s\n",
      "   200K .......... .......... .......... .......... ..........  3% 7.12M 5s\n",
      "   250K .......... .......... .......... .......... ..........  4% 1.54M 5s\n",
      "   300K .......... .......... .......... .......... ..........  5% 4.62M 4s\n",
      "   350K .......... .......... .......... .......... ..........  6% 5.13M 4s\n",
      "   400K .......... .......... .......... .......... ..........  7% 5.84M 3s\n",
      "   450K .......... .......... .......... .......... ..........  7% 3.34M 3s\n",
      "   500K .......... .......... .......... .......... ..........  8% 4.19M 3s\n",
      "   550K .......... .......... .......... .......... ..........  9% 2.84M 3s\n",
      "   600K .......... .......... .......... .......... .......... 10% 5.05M 3s\n",
      "   650K .......... .......... .......... .......... .......... 10% 5.55M 3s\n",
      "   700K .......... .......... .......... .......... .......... 11% 5.30M 2s\n",
      "   750K .......... .......... .......... .......... .......... 12% 5.91M 2s\n",
      "   800K .......... .......... .......... .......... .......... 13% 2.78M 2s\n",
      "   850K .......... .......... .......... .......... .......... 14% 4.97M 2s\n",
      "   900K .......... .......... .......... .......... .......... 14% 5.05M 2s\n",
      "   950K .......... .......... .......... .......... .......... 15% 6.56M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 16% 2.74M 2s\n",
      "  1050K .......... .......... .......... .......... .......... 17% 2.05M 2s\n",
      "  1100K .......... .......... .......... .......... .......... 18% 1.40M 2s\n",
      "  1150K .......... .......... .......... .......... .......... 18% 11.1M 2s\n",
      "  1200K .......... .......... .......... .......... .......... 19% 4.95M 2s\n",
      "  1250K .......... .......... .......... .......... .......... 20% 4.94M 2s\n",
      "  1300K .......... .......... .......... .......... .......... 21% 3.70M 2s\n",
      "  1350K .......... .......... .......... .......... .......... 21% 1.67M 2s\n",
      "  1400K .......... .......... .......... .......... .......... 22% 3.53M 2s\n",
      "  1450K .......... .......... .......... .......... .......... 23% 2.76M 2s\n",
      "  1500K .......... .......... .......... .......... .......... 24% 4.27M 2s\n",
      "  1550K .......... .......... .......... .......... .......... 25% 4.71M 2s\n",
      "  1600K .......... .......... .......... .......... .......... 25% 1.82M 2s\n",
      "  1650K .......... .......... .......... .......... .......... 26% 4.84M 2s\n",
      "  1700K .......... .......... .......... .......... .......... 27% 2.34M 2s\n",
      "  1750K .......... .......... .......... .......... .......... 28% 3.85M 2s\n",
      "  1800K .......... .......... .......... .......... .......... 29% 4.14M 2s\n",
      "  1850K .......... .......... .......... .......... .......... 29%  861K 2s\n",
      "  1900K .......... .......... .......... .......... .......... 30% 17.0M 2s\n",
      "  1950K .......... .......... .......... .......... .......... 31% 5.07M 2s\n",
      "  2000K .......... .......... .......... .......... .......... 32% 10.0M 2s\n",
      "  2050K .......... .......... .......... .......... .......... 32% 5.16M 2s\n",
      "  2100K .......... .......... .......... .......... .......... 33% 5.97M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 34% 3.67M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 35% 5.37M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 36% 10.1M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 36% 5.23M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 37% 5.80M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 38% 5.16M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 39% 5.49M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 39% 8.60M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 40% 5.44M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 41% 5.27M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 42% 6.70M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 43% 4.36M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 43% 4.69M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 44% 11.2M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 45% 4.01M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 46% 5.67M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 47% 2.44M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 47% 5.41M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 48% 5.26M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 49% 5.76M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 50% 4.61M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 50% 3.29M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 51% 4.88M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 52% 7.17M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 53% 5.10M 1s\n",
      "  3400K .......... .......... .......... .......... .......... 54% 6.12M 1s\n",
      "  3450K .......... .......... .......... .......... .......... 54% 6.37M 1s\n",
      "  3500K .......... .......... .......... .......... .......... 55% 5.12M 1s\n",
      "  3550K .......... .......... .......... .......... .......... 56% 6.56M 1s\n",
      "  3600K .......... .......... .......... .......... .......... 57% 2.58M 1s\n",
      "  3650K .......... .......... .......... .......... .......... 58% 5.46M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 58% 5.62M 1s\n",
      "  3750K .......... .......... .......... .......... .......... 59% 3.54M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 60% 5.83M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 61% 4.97M 1s\n",
      "  3900K .......... .......... .......... .......... .......... 61% 7.25M 1s\n",
      "  3950K .......... .......... .......... .......... .......... 62% 6.81M 1s\n",
      "  4000K .......... .......... .......... .......... .......... 63% 5.64M 1s\n",
      "  4050K .......... .......... .......... .......... .......... 64% 4.49M 1s\n",
      "  4100K .......... .......... .......... .......... .......... 65% 5.51M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 65% 8.89M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 66% 4.08M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 67% 7.16M 1s\n",
      "  4300K .......... .......... .......... .......... .......... 68% 4.41M 1s\n",
      "  4350K .......... .......... .......... .......... .......... 68% 4.55M 1s\n",
      "  4400K .......... .......... .......... .......... .......... 69% 5.33M 1s\n",
      "  4450K .......... .......... .......... .......... .......... 70% 6.36M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 71% 9.59M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 72% 3.92M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 72% 5.76M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 73% 10.0M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 74% 5.25M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 75% 5.75M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 76% 5.59M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 76% 4.47M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 77% 4.92M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 78% 4.70M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 79% 3.02M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 79% 4.27M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 80% 3.95M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 81% 5.63M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 82% 5.27M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 83% 4.62M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 83% 4.80M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 84% 4.91M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 85% 5.53M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 86% 5.14M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 87% 10.5M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 87% 5.24M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 88% 5.68M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 89% 5.22M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 90% 5.44M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 90% 10.3M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 91% 5.19M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 92% 5.09M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 93% 6.67M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 94% 2.64M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 94% 2.03M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 95% 8.90M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 96% 2.49M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 97% 4.84M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 97% 6.86M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 98% 4.96M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 99% 5.18M 0s\n",
      "  6350K .......... .......... .........                       100% 1.98M=1.6s\n",
      "\n",
      "2022-03-19 22:17:11 (3.94 MB/s) - 'fra-eng.zip' saved [6532197/6532197]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd107af2",
   "metadata": {},
   "source": [
    "## Extracting the Zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6446848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ = zipfile.ZipFile('fra-eng.zip')\n",
    "zip_.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4011e",
   "metadata": {},
   "source": [
    "## Import required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da38cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,re\n",
    "from unicodedata import normalize\n",
    "from numpy import array,argmax\n",
    "from pickle import load,dump\n",
    "from numpy.random import rand,shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
    "from nltk.translate.bleu_score import SmoothingFunction,corpus_bleu\n",
    "smoothie = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e653459",
   "metadata": {},
   "source": [
    "## Loading the file and reading the content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf141c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file into memory\n",
    "\n",
    "def load_file(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec86a8",
   "metadata": {},
   "source": [
    "## Splitting the sentence into pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95755d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "\n",
    "def splitting_sentence(doc):\n",
    "    sentences = doc.strip().split('\\n')\n",
    "    pairs = [sentence.split('\\t') for sentence in  sentences]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5550b8",
   "metadata": {},
   "source": [
    "## Cleaning the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "518f5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning a list of sentences and creating pairs\n",
    "\n",
    "def clean_pairs(sentences):\n",
    "    cleaned = list()\n",
    " \n",
    "    # preparing regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "    # preparing translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # iterating over each pair\n",
    "    for pair in sentences:\n",
    "        clean_pair = list()\n",
    "  \n",
    "        for sentence in pair:\n",
    "            # normalizing unicode characters\n",
    "            sentence = normalize('NFD', sentence).encode('ascii', 'ignore')\n",
    "            sentence = sentence.decode('UTF-8')\n",
    "            # tokenizing on white space\n",
    "            sentence = sentence.split()\n",
    "            # converting to lowercase\n",
    "            sentence = [word.lower() for word in sentence]\n",
    "            # removing punctuation from each token\n",
    "            sentence = [word.translate(table) for word in sentence]\n",
    "            # removing non-printable chars form each token\n",
    "            sentence = [re_print.sub('', w) for w in sentence]\n",
    "            # removing tokens with numbers in them\n",
    "            sentence = [word for word in sentence if word.isalpha()]\n",
    "            # storing as string\n",
    "            clean_pair.append(' '.join(sentence))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79de9e3",
   "metadata": {},
   "source": [
    "## Saving the Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1bcc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print(filename,': Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a1b83",
   "metadata": {},
   "source": [
    "## Saving data in .pkl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17c1c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-french.pkl : Saved\n",
      "English --> French\n",
      "go --> va\n",
      "go --> marche\n",
      "go --> bouge\n",
      "hi --> salut\n",
      "hi --> salut\n",
      "run --> cours\n",
      "run --> courez\n",
      "run --> prenez vos jambes a vos cous\n",
      "run --> file\n",
      "run --> filez\n",
      "run --> cours\n",
      "run --> fuyez\n",
      "run --> fuyons\n",
      "run --> cours\n",
      "run --> courez\n",
      "run --> prenez vos jambes a vos cous\n",
      "run --> file\n",
      "run --> filez\n",
      "run --> cours\n",
      "run --> fuyez\n",
      "run --> fuyons\n",
      "who --> qui\n",
      "wow --> ca alors\n",
      "duck --> a terre\n",
      "duck --> baissetoi\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "filename = 'fra.txt'\n",
    "doc = load_file(filename)\n",
    "\n",
    "# split into english-french pairs\n",
    "pairs = splitting_sentence(doc)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "\n",
    "# save clean pairs to file\n",
    "saving_clean_data(clean_pairs, 'english-french.pkl')\n",
    "\n",
    "print('English','-->',\"French\")\n",
    "# spot check\n",
    "for i in range(25):\n",
    "    print(clean_pairs[i,0],'-->',clean_pairs[i,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad685510",
   "metadata": {},
   "source": [
    "## Loading the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9e3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def loading_cleaned_data(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d7f5a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192341, 3)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = loading_cleaned_data('english-french.pkl')\n",
    "print(data.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c26fb",
   "metadata": {},
   "source": [
    "### Scaling of data\n",
    "\n",
    "### Size\n",
    "\n",
    " 1.Dataset - 20000\n",
    "\n",
    " 2.Training - 18000\n",
    "\n",
    " 3.Testing - 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4c059c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-french-both.pkl : Saved\n",
      "english-french-train.pkl : Saved\n",
      "english-french-test.pkl : Saved\n"
     ]
    }
   ],
   "source": [
    "# reducing dataset size (scaling) \n",
    "\n",
    "new_data_size = 20000\n",
    "dataset = data[:new_data_size, :]\n",
    "\n",
    "# randomly shuffling the dataset to get proper training and testing data\n",
    "shuffle(dataset)\n",
    "\n",
    "# splitting into training and testing (90%-10%)\n",
    "train, test = dataset[:18000], dataset[18000:]\n",
    "\n",
    "# saving the cleaned data,train data and test data \n",
    "saving_clean_data(dataset, 'english-french-both.pkl')\n",
    "saving_clean_data(train, 'english-french-train.pkl')\n",
    "saving_clean_data(test, 'english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba74307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets and saving it into variables\n",
    "dataset = loading_cleaned_data('english-french-both.pkl')\n",
    "train = loading_cleaned_data('english-french-train.pkl')\n",
    "test = loading_cleaned_data('english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7df3a",
   "metadata": {},
   "source": [
    "## Creating a tokenizer for the lines and finding the maximum length phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9df5bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e69c4",
   "metadata": {},
   "source": [
    "## Size of English & French vocabulary and their max phrase length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53101000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3418\n",
      "English Max Length: 5\n",
      "French Vocabulary Size: 6977\n",
      "French Max Length: 11\n"
     ]
    }
   ],
   "source": [
    "# preparing the english tokenizer\n",
    "\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# preparing the french tokenizer\n",
    "\n",
    "fra_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(dataset[:, 1])\n",
    "print('French Vocabulary Size: %d' % fra_vocab_size)\n",
    "print('French Max Length: %d' % (fra_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d6fc3",
   "metadata": {},
   "source": [
    "## Encoding to integers and padding to the maximum phrase length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd2eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Output sequence must be encoded to integers and padded to the maximum phrase length\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    x = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    x = pad_sequences(x, maxlen=length, padding='post')\n",
    "    return x\n",
    "\n",
    "# One hot encoding to max phrase length\n",
    "def one_hot_encoding(sequences, vocab_size):\n",
    "    y_1 = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        y_1.append(encoded)\n",
    "    y = array(y_1)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b0ea8",
   "metadata": {},
   "source": [
    "## Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ce4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training data\n",
    "trainX = encode_sequences(fra_tokenizer, fra_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = one_hot_encoding(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare testing data\n",
    "testX = encode_sequences(fra_tokenizer, fra_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer,eng_length, test[:, 0])\n",
    "testY = one_hot_encoding(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81cade8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (18000, 11) (18000, 5, 3418)\n",
      "testing size: (2000, 11) (2000, 5, 3418)\n"
     ]
    }
   ],
   "source": [
    "print('training size:',trainX.shape,trainY.shape)\n",
    "print('testing size:',testX.shape,testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa895fe",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d189b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(source_vocab, target_vocab, source_len, target_len, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(source_vocab, units, input_length=source_len, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(target_len))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(target_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547545c9",
   "metadata": {},
   "source": [
    "## Defining and Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5ac1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_building(fra_vocab_size, eng_vocab_size, fra_length, eng_length, 512)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f005e94",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e380856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 11, 512)           3572224   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 512)               2099200   \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 5, 512)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 5, 512)            2099200   \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 5, 3418)          1753434   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,524,058\n",
      "Trainable params: 9,524,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9577cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop model if accuracy of the model doesn't changes by more than 0.01 \n",
    "# Patience = 5 : After each 5 epochs if no improvement is there then training will be stopped.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852c7ba",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "1.Epochs = 50\n",
    "\n",
    "2.Batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8acdbc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "720/720 - 194s - loss: 3.6283 - acc: 0.4596 - val_loss: 3.1087 - val_acc: 0.5107 - 194s/epoch - 269ms/step\n",
      "Epoch 2/50\n",
      "720/720 - 190s - loss: 2.7595 - acc: 0.5493 - val_loss: 2.5955 - val_acc: 0.5834 - 190s/epoch - 264ms/step\n",
      "Epoch 3/50\n",
      "720/720 - 190s - loss: 2.2167 - acc: 0.6061 - val_loss: 2.2646 - val_acc: 0.6197 - 190s/epoch - 264ms/step\n",
      "Epoch 4/50\n",
      "720/720 - 190s - loss: 1.7816 - acc: 0.6512 - val_loss: 2.0586 - val_acc: 0.6447 - 190s/epoch - 264ms/step\n",
      "Epoch 5/50\n",
      "720/720 - 190s - loss: 1.4156 - acc: 0.6977 - val_loss: 1.8979 - val_acc: 0.6661 - 190s/epoch - 264ms/step\n",
      "Epoch 6/50\n",
      "720/720 - 196s - loss: 1.0952 - acc: 0.7466 - val_loss: 1.7770 - val_acc: 0.6898 - 196s/epoch - 272ms/step\n",
      "Epoch 7/50\n",
      "720/720 - 202s - loss: 0.8333 - acc: 0.7957 - val_loss: 1.7125 - val_acc: 0.7027 - 202s/epoch - 280ms/step\n",
      "Epoch 8/50\n",
      "720/720 - 222s - loss: 0.6288 - acc: 0.8377 - val_loss: 1.6763 - val_acc: 0.7135 - 222s/epoch - 308ms/step\n",
      "Epoch 9/50\n",
      "720/720 - 214s - loss: 0.4815 - acc: 0.8712 - val_loss: 1.6416 - val_acc: 0.7240 - 214s/epoch - 298ms/step\n",
      "Epoch 10/50\n",
      "720/720 - 188s - loss: 0.3734 - acc: 0.8981 - val_loss: 1.6485 - val_acc: 0.7272 - 188s/epoch - 260ms/step\n",
      "Epoch 11/50\n",
      "720/720 - 184s - loss: 0.3009 - acc: 0.9150 - val_loss: 1.6711 - val_acc: 0.7309 - 184s/epoch - 255ms/step\n",
      "Epoch 12/50\n",
      "720/720 - 184s - loss: 0.2551 - acc: 0.9256 - val_loss: 1.6855 - val_acc: 0.7346 - 184s/epoch - 255ms/step\n",
      "Epoch 13/50\n",
      "720/720 - 183s - loss: 0.2251 - acc: 0.9308 - val_loss: 1.6966 - val_acc: 0.7345 - 183s/epoch - 255ms/step\n",
      "Epoch 14/50\n",
      "720/720 - 183s - loss: 0.2025 - acc: 0.9370 - val_loss: 1.7259 - val_acc: 0.7387 - 183s/epoch - 254ms/step\n",
      "Epoch 15/50\n",
      "720/720 - 183s - loss: 0.1938 - acc: 0.9384 - val_loss: 1.7295 - val_acc: 0.7382 - 183s/epoch - 254ms/step\n",
      "Epoch 16/50\n",
      "720/720 - 186s - loss: 0.1840 - acc: 0.9396 - val_loss: 1.7521 - val_acc: 0.7347 - 186s/epoch - 259ms/step\n",
      "Epoch 17/50\n",
      "720/720 - 183s - loss: 0.1760 - acc: 0.9413 - val_loss: 1.7818 - val_acc: 0.7378 - 183s/epoch - 254ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ac64bd160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(trainX, trainY, epochs= 50, batch_size=25, validation_data=(testX, testY), verbose=2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ee237",
   "metadata": {},
   "source": [
    "## Evaluating model and calculating BLEU Score\n",
    "\n",
    "Evaluation involves two steps:\n",
    "\n",
    "1.Generating a translated output sequence, and\n",
    "\n",
    "2.then repeating this process for many input examples and summarizing the skill of the model across multiple cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aec41185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33d0546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd408985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "  \n",
    "    # Creating empty lists for actual phrases(French) and predicted phrases(English) \n",
    "    actual,predicted = list(),list()\n",
    "    a,b,c = list(),list(),list()\n",
    "    for i,source in enumerate(sources):\n",
    "        # reshaping to the required size\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "\n",
    "        # predicting for the english tokenizer\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        # raw_dataset = raw_dataset[i].split(' ') \n",
    "        # print(raw_dataset[i][1])\n",
    "\n",
    "        raw_src,raw_target = raw_dataset[i][1],raw_dataset[i][0]\n",
    "    \n",
    "        # First 10 Predictions\n",
    "        if i <= 10:\n",
    "            print('source = ',raw_src,'<--->', ' target = ',raw_target,'<--->','  predicted = ',translation)\n",
    "\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "  \n",
    "    # calculating BLEU score\n",
    "    print('-------------------------------------------')\n",
    "    print('BLEU Score :')\n",
    "    print('BLEU score-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "    print('BLEU score-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "    print('BLEU score-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "    print('BLEU score-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie,auto_reweigh=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c33e9f",
   "metadata": {},
   "source": [
    "## Evaluating Model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e718432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source =  on se voit demain <--->  target =  see you tomorrow <--->   predicted =  see you tomorrow\n",
      "source =  je vais aller a heures <--->  target =  i will go at <--->   predicted =  i will go\n",
      "source =  recommencez <--->  target =  start over <--->   predicted =  start over\n",
      "source =  prenez de la pizza <--->  target =  have some pizza <--->   predicted =  have some pizza\n",
      "source =  cetait net <--->  target =  it was clean <--->   predicted =  it was clean\n",
      "source =  tom vient juste de venir <--->  target =  tom just came <--->   predicted =  tom just came\n",
      "source =  cest un tenancier de bar <--->  target =  hes a bartender <--->   predicted =  hes a bartender\n",
      "source =  tom a bien combattu <--->  target =  tom fought well <--->   predicted =  tom fought well\n",
      "source =  emmene la cle <--->  target =  bring the key <--->   predicted =  bring the key\n",
      "source =  nous sommes populaires <--->  target =  were popular <--->   predicted =  were popular\n",
      "source =  cest mon destin <--->  target =  this is my fate <--->   predicted =  this is my fate\n",
      "-------------------------------------------\n",
      "BLEU Score :\n",
      "BLEU score-1: 0.935872\n",
      "BLEU score-2: 0.916927\n",
      "BLEU score-3: 0.855072\n",
      "BLEU score-4: 0.599591\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model,eng_tokenizer,trainX,train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57734e2f",
   "metadata": {},
   "source": [
    "## Evaluating Model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a261600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source =  puisje avoir un calin <--->  target =  can i have a hug <--->   predicted =  can i go this\n",
      "source =  descends <--->  target =  come on down <--->   predicted =  get down\n",
      "source =  jai demande a tom <--->  target =  i asked tom <--->   predicted =  i called tom\n",
      "source =  je vous ferai un proces <--->  target =  ill sue you <--->   predicted =  i will you you\n",
      "source =  cest faisable <--->  target =  thats doable <--->   predicted =  this is doable\n",
      "source =  je vous ai desobei <--->  target =  i disobeyed you <--->   predicted =  i disobeyed you\n",
      "source =  estce un elan <--->  target =  is it an elk <--->   predicted =  is that a a\n",
      "source =  je vous en prie <--->  target =  you go first <--->   predicted =  i beg you\n",
      "source =  ne le niez pas <--->  target =  dont deny it <--->   predicted =  dont deny it\n",
      "source =  les hommes sont des idiots <--->  target =  men are idiots <--->   predicted =  men are pigs\n",
      "source =  je leur souhaite bonne chance <--->  target =  i wish them luck <--->   predicted =  i have a good\n",
      "-------------------------------------------\n",
      "BLEU Score :\n",
      "BLEU score-1: 0.615169\n",
      "BLEU score-2: 0.515628\n",
      "BLEU score-3: 0.453464\n",
      "BLEU score-4: 0.270446\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef0421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
